{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Word Tokenizer</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.1. Training the Word Tokenizer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHANGE YOUR PATH HERE.\n",
    "drive_path = '/Users/kaganhitit_/Desktop/COMP442/HW1/starter/'\n",
    "##drive_path = \"path/to/your/directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing articles...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 151\u001b[0m\n\u001b[1;32m    148\u001b[0m vocab_save_path \u001b[38;5;241m=\u001b[39m drive_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings/word_tokenizer_vocab.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    150\u001b[0m articles \u001b[38;5;241m=\u001b[39m read_articles(training_set_path)\n\u001b[0;32m--> 151\u001b[0m tokenized_articles, vocabulary \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Convert the set of words to a dictionary with indices\u001b[39;00m\n\u001b[1;32m    154\u001b[0m vocab_dict \u001b[38;5;241m=\u001b[39m {word: i \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(vocabulary)}\n",
      "Cell \u001b[0;32mIn[5], line 105\u001b[0m, in \u001b[0;36mword_tokenizer\u001b[0;34m(articles)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03mPerforms word tokenization on Turkish articles.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    A list containing tokenized articles (lists of tokens) and the vocabulary (set of tokens).\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokenizing articles...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 105\u001b[0m processed_articles \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_turkish_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m tokenized_articles \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m processed_articles:\n",
      "Cell \u001b[0;32mIn[5], line -1\u001b[0m, in \u001b[0;36mfilter_turkish_text\u001b[0;34m(articles)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def read_articles(file_path, max_articles=400000):\n",
    "    \"\"\"\n",
    "    Reads articles from a text file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the text file containing articles.\n",
    "        max_articles (int, optional): Maximum number of articles to read. Defaults to 400000.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary represents an article with keys \"title\" and \"text\".\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding='utf-8') as fi:\n",
    "        content = fi.read()\n",
    "\n",
    "    titles = re.findall(\"== .* ==\", content)\n",
    "    texts = re.split(\"== .* == \\n\\n\", content)[1:]\n",
    "    articles = [{\"title\": ti.strip(\"==\").strip(), \"text\": tx.strip()} for ti, tx in list(zip(titles, texts))[:max_articles]]\n",
    "    return articles\n",
    "\n",
    "def filter_vocabulary(tokenized_articles, vocabulary_count=50000):\n",
    "    \"\"\"\n",
    "    Filters vocabulary based on token frequency.\n",
    "\n",
    "    Args:\n",
    "        tokenized_articles (list): A list of lists, where each inner list represents the tokens of an article.\n",
    "        vocabulary_count (int, optional): The maximum number of tokens to keep in the vocabulary. Defaults to 50000.\n",
    "\n",
    "    Returns:\n",
    "        A list of filtered articles and a set of the most frequent tokens.\n",
    "    \"\"\"\n",
    "    print('Filtering tokens according to vocabulary count:', vocabulary_count)\n",
    "    all_tokens = [token for article in tokenized_articles for token in article]\n",
    "    token_freq = Counter(all_tokens)\n",
    "\n",
    "    most_frequent_tokens = set([token for token, _ in token_freq.most_common(vocabulary_count - 1)])\n",
    "    most_frequent_tokens.add('<OOV>')\n",
    "\n",
    "    filtered_articles = []\n",
    "    for article in tokenized_articles:\n",
    "        filtered_article = [token if token in most_frequent_tokens else '<OOV>' for token in article]\n",
    "        filtered_articles.append(filtered_article)\n",
    "\n",
    "    print('Filtering finished.')\n",
    "    return filtered_articles, most_frequent_tokens\n",
    "\n",
    "def turkish_lower(text):\n",
    "    \"\"\"\n",
    "    Converts Turkish text to lowercase and handles special characters.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to convert.\n",
    "\n",
    "    Returns:\n",
    "        str: The converted text in lowercase.\n",
    "    \"\"\"\n",
    "    text = text.replace('I', 'ı').replace('İ', 'i')\n",
    "    return text.lower()\n",
    "\n",
    "def filter_turkish_text(articles):\n",
    "    \"\"\"\n",
    "    Filters and processes Turkish text articles.\n",
    "\n",
    "    Args:\n",
    "        articles (list): A list of dictionaries, where each dictionary represents an article with a \"text\" key.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of processed text strings from the articles.\n",
    "    \"\"\"\n",
    "    # Regex pattern for Turkish characters, excluding single characters\n",
    "    token_pattern = re.compile(r'\\b[a-zğüşıöç]{2,}(?:\\'[a-zğüşıöç]+)?\\b')\n",
    "\n",
    "    processed_articles = []\n",
    "\n",
    "    stop_words = import_stop_words(drive_path + 'data/turkce-stop-words.txt')\n",
    "\n",
    "    for article in articles:\n",
    "        text = turkish_lower(article['text'])\n",
    "        tokens = token_pattern.findall(text)\n",
    "        for token in tokens:\n",
    "            if(is_stop_word(stop_words, token)):\n",
    "                tokens.remove(token)\n",
    "        filtered_text = ' '.join(tokens)\n",
    "        processed_articles.append(filtered_text)\n",
    "\n",
    "    print(processed_articles[0])\n",
    "    print('Articles are processed.')\n",
    "    return processed_articles\n",
    "\n",
    "def word_tokenizer(articles):\n",
    "    \"\"\"\n",
    "    Performs word tokenization on Turkish articles.\n",
    "\n",
    "    Args:\n",
    "        articles (list): A list of dictionaries, where each dictionary represents an article with a \"text\" key.\n",
    "\n",
    "    Returns:\n",
    "        A list containing tokenized articles (lists of tokens) and the vocabulary (set of tokens).\n",
    "    \"\"\"\n",
    "    print('Tokenizing articles...')\n",
    "\n",
    "    processed_articles = filter_turkish_text(articles)\n",
    "\n",
    "    tokenized_articles = []\n",
    "\n",
    "    for article in processed_articles:\n",
    "        tokens = article.split(' ')\n",
    "        tokenized_articles.append(tokens)\n",
    "    print(tokenized_articles[0])\n",
    "    tokenized_articles, vocabulary = filter_vocabulary(tokenized_articles)\n",
    "    print('Articles are tokenized.')\n",
    "    return tokenized_articles, vocabulary\n",
    "\n",
    "def import_stop_words(file_path):\n",
    "    \"\"\"\n",
    "    Imports stop words from a text file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the text file containing stop words (one word per line).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of stop words.\n",
    "    \"\"\"\n",
    "    stop_words = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            stop_words.append(line.strip())  # .strip() removes any trailing newlines or spaces\n",
    "    return stop_words\n",
    "\n",
    "def is_stop_word(stop_words,word):\n",
    "    \"\"\"\n",
    "    Checks if a word is a stop word.\n",
    "\n",
    "    Args:\n",
    "        stop_words (list): A list of stop words.\n",
    "        word (str): The word to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the word is a stop word, False otherwise.\n",
    "    \"\"\"\n",
    "    return word in stop_words\n",
    "\n",
    "## Reading the articles\n",
    "training_set_path = drive_path + \"data/trwiki-67/trwiki-67.train.txt\"\n",
    "vocab_save_path = drive_path + 'embeddings/word_tokenizer_vocab.json'\n",
    "\n",
    "articles = read_articles(training_set_path)\n",
    "tokenized_articles, vocabulary = word_tokenizer(articles)\n",
    "\n",
    "# Convert the set of words to a dictionary with indices\n",
    "vocab_dict = {word: i for i, word in enumerate(vocabulary)}\n",
    "\n",
    "with open(vocab_save_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"{\\n\")\n",
    "    for i, (word, index) in enumerate(vocab_dict.items()):\n",
    "        # Add a comma at the end of each line except the last one\n",
    "        separator = \",\" if i < len(vocab_dict) - 1 else \"\"\n",
    "        f.write(f'    \"{word}\": {index}{separator}\\n')\n",
    "    f.write(\"}\")\n",
    "\n",
    "print(f\"Vocabulary saved at {vocab_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.2. Training the Word Embeddings</h2>\n",
    "<h4>Content:</h4>\n",
    "<l>\n",
    "    <li>1.2.1.Defining the functions and training model</li>\n",
    "    <li>1.2.2.Preparing the training and validation dataset</li>\n",
    "    <li>1.2.3.Training loop</li>\n",
    "</l>\n",
    "\n",
    "<h3>1.2.1. Defining the functions and training model</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SGD\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import json\n",
    "\n",
    "def read_articles(file_path, max_articles=400000):\n",
    "    with open(file_path, \"r\", encoding='utf-8') as fi:\n",
    "        content = fi.read()\n",
    "\n",
    "    # Find all article titles and texts\n",
    "    titles = re.findall(\"== .* ==\", content)\n",
    "    texts = re.split(\"== .* == \\n\\n\", content)[1:]\n",
    "\n",
    "    # Convert zipped titles and texts to a list, then slice\n",
    "    articles = [{\"title\": ti.strip(\"==\").strip(), \"text\": tx.strip()} for ti, tx in list(zip(titles, texts))[:max_articles]]\n",
    "    return articles\n",
    "\n",
    "def turkish_lower(text):\n",
    "    text = text.replace('I', 'ı').replace('İ', 'i')\n",
    "    return text.lower()\n",
    "\n",
    "def filter_turkish_text(articles):\n",
    "    print('Processing articles...')\n",
    "    # Regex pattern for Turkish characters, excluding single characters\n",
    "    token_pattern = re.compile(r'\\b[a-zğüşıöç]{2,}(?:\\'[a-zğüşıöç]+)?\\b')\n",
    "\n",
    "    processed_articles = []\n",
    "\n",
    "    stop_words = import_stop_words(drive_path + 'data/turkce-stop-words.txt')\n",
    "\n",
    "    for article in articles:\n",
    "        text = turkish_lower(article['text'])\n",
    "        tokens = token_pattern.findall(text)\n",
    "        for token in tokens:\n",
    "            if(is_stop_word(stop_words, token)):\n",
    "                tokens.remove(token)\n",
    "        filtered_text = ' '.join(tokens)\n",
    "        processed_articles.append(filtered_text)\n",
    "\n",
    "    print(processed_articles[0])\n",
    "    print('Articles are processed.')\n",
    "    return processed_articles\n",
    "\n",
    "def tokenize_articles(articles, vocabulary):\n",
    "  processed_articles = filter_turkish_text(articles)\n",
    "\n",
    "  tokenized_articles = []\n",
    "  for article in processed_articles:\n",
    "      # Tokenize each article's text\n",
    "      tokens = article.split(' ')\n",
    "      tokenized_articles.append(tokens)\n",
    "  return tokenized_articles\n",
    "\n",
    "def load_vocabulary(vocab_path):\n",
    "  with open(vocab_path, 'r', encoding='utf-8') as f:\n",
    "      vocab = json.load(f)\n",
    "  return vocab\n",
    "\n",
    "def tokens_to_indices(tokenized_articles, vocabulary):\n",
    "    for article_index in range(len(tokenized_articles)):\n",
    "        for token_index in range(len(tokenized_articles[article_index])):\n",
    "            token = tokenized_articles[article_index][token_index]\n",
    "            tokenized_articles[article_index][token_index] = vocabulary.get(token, vocabulary.get('<OOV>'))\n",
    "    return tokenized_articles\n",
    "\n",
    "def create_tuples_from_articles(indexed_articles, context_size):\n",
    "    context_target_pairs = []\n",
    "    for article in indexed_articles:\n",
    "        # Add padding at the beginning and end of each article\n",
    "        for i in range(context_size, len(article) - context_size):\n",
    "            context = article[i - context_size:i] + article[i + 1:i + context_size + 1]\n",
    "            target = article[i]\n",
    "            context_target_pairs.append((context, target))\n",
    "    return context_target_pairs\n",
    "\n",
    "def import_stop_words(file_path):\n",
    "    stop_words = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            stop_words.append(line.strip())  # .strip() removes any trailing newlines or spaces\n",
    "    return stop_words\n",
    "\n",
    "def is_stop_word(stop_words,word):\n",
    "    return word in stop_words\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, max_norm = 1.0)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds_mean = embeds.mean(dim=1)\n",
    "        out = self.linear(embeds_mean)\n",
    "        return out\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, context_target_pairs):\n",
    "        self.context_target_pairs = context_target_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.context_target_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.context_target_pairs[idx]\n",
    "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.2.2. Preparing the training and validation dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the articles\n",
    "training_set_path = drive_path + \"data/trwiki-67/trwiki-67.train.txt\"\n",
    "validation_set_path = drive_path + \"data/trwiki-67/trwiki-67.val.txt\"\n",
    "tokenizer_path = drive_path + 'embeddings/word_tokenizer_vocab.json'  # Path to the saved tokenizer\n",
    "\n",
    "## Reading the tokenizer.\n",
    "vocabulary = load_vocabulary(tokenizer_path)\n",
    "\n",
    "## Tokenized training and validation sets. The arrays are indexes, not words.\n",
    "training_articles = tokens_to_indices(tokenize_articles(read_articles(training_set_path), vocabulary), vocabulary)\n",
    "validation_articles = tokens_to_indices(tokenize_articles(read_articles(validation_set_path), vocabulary), vocabulary)\n",
    "\n",
    "CONTEXT_SIZE = 2\n",
    "training_articles = create_tuples_from_articles(training_articles, CONTEXT_SIZE)\n",
    "validation_articles = create_tuples_from_articles(validation_articles, CONTEXT_SIZE)\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = CBOWDataset(training_articles)\n",
    "val_dataset = CBOWDataset(validation_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.2.3. Training loop</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 512\n",
    "learning_rate = 0.04\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "vocab_size = len(vocabulary.keys())\n",
    "\n",
    "print('Vocab size : ' + str(vocab_size))\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Model, Device, DataLoader\n",
    "neural_net = CBOW(vocab_size, embedding_dim)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "neural_net.to(device)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(neural_net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "print('Starting training')\n",
    "for epoch in range(epochs):\n",
    "    neural_net.train()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "\n",
    "    for context_vector, target_index in progress_bar:\n",
    "        context_vector, target_index = context_vector.to(device), target_index.to(device)\n",
    "\n",
    "        neural_net.zero_grad()\n",
    "        log_probs = neural_net(context_vector)\n",
    "        loss = loss_function(log_probs, target_index)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f'\\nEpoch {epoch+1} Completed - Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    # Validation\n",
    "    neural_net.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for context_vector, target_index in val_dataloader:\n",
    "            context_vector, target_index = context_vector.to(device), target_index.to(device)\n",
    "            log_probs = neural_net(context_vector)\n",
    "            loss = loss_function(log_probs, target_index)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Save model\n",
    "    model_save_path = drive_path + f'word_model_epoch_{epoch+1}.pth'\n",
    "    torch.save(neural_net.state_dict(), model_save_path)\n",
    "\n",
    "# Optionally, save the final model separatelys\n",
    "final_model_save_path = drive_path + 'word_model_final.pth'\n",
    "torch.save(neural_net.state_dict(), final_model_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Character Trigram Tokenizer</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2.1. Training the Character Trigram Tokenizer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def read_articles(file_path, max_articles=400000):\n",
    "    with open(file_path, \"r\", encoding='utf-8') as fi:\n",
    "        content = fi.read()\n",
    "\n",
    "    # Find all article titles and texts\n",
    "    titles = re.findall(\"== .* ==\", content)\n",
    "    texts = re.split(\"== .* == \\n\\n\", content)[1:]\n",
    "\n",
    "    # Convert zipped titles and texts to a list, then slice\n",
    "    articles = [{\"title\": ti.strip(\"==\").strip(), \"text\": tx.strip()} for ti, tx in list(zip(titles, texts))[:max_articles]]\n",
    "    return articles\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def filter_vocabulary(tokenized_articles, vocabulary_count=50000):\n",
    "    print('Filtering tokens according to vocabulary count:', vocabulary_count)\n",
    "\n",
    "    all_tokens = [token for article in tokenized_articles for token in article]\n",
    "    token_freq = Counter(all_tokens)\n",
    "\n",
    "    most_frequent_tokens = set([token for token, _ in token_freq.most_common(vocabulary_count)])\n",
    "\n",
    "    filtered_articles = []\n",
    "    for article in tokenized_articles:\n",
    "        filtered_article = [token if token in most_frequent_tokens else '<OOV>' for token in article]\n",
    "        filtered_articles.append(filtered_article)\n",
    "\n",
    "    print('Filtering finished.')\n",
    "    return filtered_articles, most_frequent_tokens\n",
    "\n",
    "def turkish_lower(text):\n",
    "    text = text.replace('I', 'ı').replace('İ', 'i')\n",
    "    return text.lower()\n",
    "\n",
    "def filter_turkish_text(articles):\n",
    "    print('Processing articles...')\n",
    "    # Regex pattern for Turkish characters, excluding single characters\n",
    "    token_pattern = re.compile(r'\\b[a-zğüşıöç]{2,}(?:\\'[a-zğüşıöç]+)?\\b')\n",
    "\n",
    "    processed_articles = []\n",
    "\n",
    "    for article in articles:\n",
    "        text = turkish_lower(article['text'])\n",
    "        tokens = token_pattern.findall(text)\n",
    "        filtered_text = ' '.join(tokens)\n",
    "        processed_articles.append(filtered_text)\n",
    "\n",
    "    print(processed_articles[0])\n",
    "    print('Articles are processed.')\n",
    "    return processed_articles\n",
    "\n",
    "\n",
    "def trigram_tokenizer(articles):\n",
    "    print('Tokenizing articles...')\n",
    "\n",
    "    processed_articles = filter_turkish_text(articles)\n",
    "\n",
    "    tokenized_articles = []\n",
    "\n",
    "    for article in processed_articles:\n",
    "        tokens = article.split(' ')\n",
    "        tokenized_words = []\n",
    "        for word in tokens:\n",
    "          word = '<' + word + '>'\n",
    "          trigrams = [word[i:i+3] for i in range(len(word) - 2)]\n",
    "          tokenized_words.extend(trigrams)\n",
    "        tokenized_articles.append(tokenized_words)\n",
    "    print(tokenized_articles[0])\n",
    "    tokenized_articles, vocabulary = filter_vocabulary(tokenized_articles)\n",
    "    print('Articles are tokenized.')\n",
    "    return tokenized_articles, vocabulary\n",
    "\n",
    "## Reading the articles\n",
    "training_set_path = drive_path + \"data/trwiki-67/trwiki-67.train.txt\"\n",
    "vocab_save_path = drive_path + 'embeddings/trigram_tokenizer_vocab.json'\n",
    "\n",
    "articles = read_articles(training_set_path)\n",
    "tokenized_articles, vocabulary = trigram_tokenizer(articles)\n",
    "\n",
    "# Convert the set of words to a dictionary with indices\n",
    "vocab_dict = {word: i for i, word in enumerate(vocabulary)}\n",
    "\n",
    "with open(vocab_save_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"{\\n\")\n",
    "    for i, (word, index) in enumerate(vocab_dict.items()):\n",
    "        # Add a comma at the end of each line except the last one\n",
    "        separator = \",\" if i < len(vocab_dict) - 1 else \"\"\n",
    "        f.write(f'    \"{word}\": {index}{separator}\\n')\n",
    "    f.write(\"}\")\n",
    "\n",
    "print(f\"Vocabulary saved at {vocab_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2.2. Training the Character Trigram Embeddings</h2>\n",
    "<h4>Content:</h4>\n",
    "<l>\n",
    "    <li>2.2.1.Defining the functions and training model</li>\n",
    "    <li>2.2.2.Preparing the training and validation dataset</li>\n",
    "    <li>2.2.3.Training loop</li>\n",
    "</l>\n",
    "\n",
    "\n",
    "<h3>2.2.1. Defining the functions and training model</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SGD\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import json\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def read_articles(file_path, max_articles=100000):\n",
    "    with open(file_path, \"r\", encoding='utf-8') as fi:\n",
    "        content = fi.read()\n",
    "\n",
    "    # Find all article titles and texts\n",
    "    titles = re.findall(\"== .* ==\", content)\n",
    "    texts = re.split(\"== .* == \\n\\n\", content)[1:]\n",
    "\n",
    "    # Convert zipped titles and texts to a list, then slice\n",
    "    articles = [{\"title\": ti.strip(\"==\").strip(), \"text\": tx.strip()} for ti, tx in list(zip(titles, texts))[:max_articles]]\n",
    "    return articles\n",
    "\n",
    "def turkish_lower(text):\n",
    "    text = text.replace('I', 'ı').replace('İ', 'i')\n",
    "    return text.lower()\n",
    "\n",
    "def filter_turkish_text(articles):\n",
    "    print('Processing articles...')\n",
    "    # Regex pattern for Turkish characters, excluding single characters\n",
    "    token_pattern = re.compile(r'\\b[a-zğüşıöç]{2,}(?:\\'[a-zğüşıöç]+)?\\b')\n",
    "\n",
    "    processed_articles = []\n",
    "\n",
    "    stop_words = import_stop_words(drive_path + 'data/turkce-stop-words.txt')\n",
    "\n",
    "    for article in articles:\n",
    "        text = turkish_lower(article['text'])\n",
    "        words = token_pattern.findall(text)\n",
    "        for word in words:\n",
    "            if(is_stop_word(stop_words, word)):\n",
    "                words.remove(word)\n",
    "            word = '<' + word + '>'\n",
    "        processed_articles.append(words)\n",
    "    print(processed_articles[0])\n",
    "    print('Articles are processed.')\n",
    "    return processed_articles\n",
    "\n",
    "def create_tuples_from_articles(processed_articles, context_size, vocabulary):\n",
    "    context_target_pairs = []\n",
    "    skipped_pairs_count = 0\n",
    "    added_pairs_count = 0\n",
    "    for article in processed_articles:\n",
    "        for i in range(context_size, len(article) - context_size):\n",
    "            # Tokenize the context and target into trigrams\n",
    "            left_context_tokens = []\n",
    "            right_context_tokens = []\n",
    "            for j in range(i - context_size, i):\n",
    "                left_context_tokens.extend(trigram_encode(article[j], vocabulary))\n",
    "            for j in range(i + 1, i + context_size + 1):\n",
    "                right_context_tokens.extend(trigram_encode(article[j], vocabulary))\n",
    "\n",
    "            target_tokens = trigram_encode(article[i], vocabulary)\n",
    "\n",
    "            # For each token in the target, create a new context-target pair\n",
    "            for target_token in target_tokens:\n",
    "                # Combine left context, other tokens of the target word, and right context\n",
    "                extended_context = left_context_tokens + \\\n",
    "                                   [token for token in target_tokens if token != target_token] + \\\n",
    "                                   right_context_tokens\n",
    "                if extended_context and target_token:\n",
    "                    context_target_pairs.append((extended_context, target_token))\n",
    "                    added_pairs_count += 1\n",
    "                else:\n",
    "                    skipped_pairs_count += 1\n",
    "    print(f\"Skipped {skipped_pairs_count} pairs.\")\n",
    "    print(f\"Added {added_pairs_count} pairs.\")\n",
    "    return context_target_pairs\n",
    "\n",
    "\n",
    "def import_stop_words(file_path):\n",
    "    stop_words = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            stop_words.append(line.strip())  # .strip() removes any trailing newlines or spaces\n",
    "    return stop_words\n",
    "\n",
    "def is_stop_word(stop_words,word):\n",
    "    return word in stop_words\n",
    "\n",
    "def load_vocabulary(vocab_path):\n",
    "  with open(vocab_path, 'r', encoding='utf-8') as f:\n",
    "      vocab = json.load(f)\n",
    "  return vocab\n",
    "\n",
    "def trigram_encode(text, vocabulary):\n",
    "    trigrams = [text[i:i+3] for i in range(len(text) - 2)]\n",
    "    encoded_trigrams = [vocabulary.get(trigram, vocabulary.get('<OOV>')) for trigram in trigrams]\n",
    "    return encoded_trigrams\n",
    "\n",
    "def get_collate_fn(pad_token_id):\n",
    "    def collate_batch(batch):\n",
    "        contexts, targets = zip(*batch)\n",
    "\n",
    "        # Since contexts are already tensors, use them directly in pad_sequence\n",
    "        contexts_padded = pad_sequence(contexts, batch_first=True, padding_value=pad_token_id)\n",
    "\n",
    "        # Convert targets list to a tensor\n",
    "        # Assuming targets are already tensors as per your dataset class\n",
    "        targets = torch.stack(targets)\n",
    "\n",
    "        return contexts_padded, targets\n",
    "\n",
    "    return collate_batch\n",
    "\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, max_norm = 1.0)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds_mean = embeds.mean(dim=1)\n",
    "        out = self.linear(embeds_mean)\n",
    "        return out\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, context_target_pairs):\n",
    "        self.context_target_pairs = context_target_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.context_target_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.context_target_pairs[idx]\n",
    "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.2.2. Preparing the training and validation dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the articles\n",
    "training_set_path = drive_path + \"data/trwiki-67/trwiki-67.train.txt\"\n",
    "validation_set_path = drive_path + \"data/trwiki-67/trwiki-67.val.txt\"\n",
    "tokenizer_path = drive_path + 'embeddings/trigram_tokenizer_vocab.json'  # Path to the saved tokenizer\n",
    "\n",
    "## Reading the tokenizer.\n",
    "vocabulary = load_vocabulary(tokenizer_path)\n",
    "\n",
    "training_articles = filter_turkish_text(read_articles(training_set_path))\n",
    "validation_articles = filter_turkish_text(read_articles(validation_set_path))\n",
    "\n",
    "CONTEXT_SIZE = 2\n",
    "training_articles = create_tuples_from_articles(training_articles, CONTEXT_SIZE, vocabulary)\n",
    "validation_articles = create_tuples_from_articles(validation_articles, CONTEXT_SIZE, vocabulary)\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = CBOWDataset(training_articles)\n",
    "val_dataset = CBOWDataset(validation_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.2.3. Training loop</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 512\n",
    "learning_rate = 0.04\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "vocab_size = len(vocabulary.keys())\n",
    "\n",
    "pad_token_id = vocabulary['<PAD>']\n",
    "print(pad_token_id)\n",
    "collate_fn = get_collate_fn(pad_token_id)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# Model, Device, DataLoader\n",
    "neural_net = CBOW(vocab_size, embedding_dim)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "neural_net.to(device)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(neural_net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "print('Starting training')\n",
    "for epoch in range(epochs):\n",
    "    neural_net.train()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "\n",
    "    for context_vector, target_index in progress_bar:\n",
    "        context_vector, target_index = context_vector.to(device), target_index.to(device)\n",
    "\n",
    "        neural_net.zero_grad()\n",
    "        log_probs = neural_net(context_vector)\n",
    "        loss = loss_function(log_probs, target_index)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f'\\nEpoch {epoch+1} Completed - Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    # Validation\n",
    "    neural_net.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for context_vector, target_index in val_dataloader:\n",
    "            context_vector, target_index = context_vector.to(device), target_index.to(device)\n",
    "            log_probs = neural_net(context_vector)\n",
    "            loss = loss_function(log_probs, target_index)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Save model\n",
    "    model_save_path = drive_path + f'trigram_model_epoch_{epoch+1}.pth'\n",
    "    torch.save(neural_net.state_dict(), model_save_path)\n",
    "\n",
    "# Optionally, save the final model separatelys\n",
    "final_model_save_path = drive_path + 'trigram_model_final.pth'\n",
    "torch.save(neural_net.state_dict(), final_model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. BPE Tokenizer</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3.1. Training the BPE Tokenizer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "def read_articles(file_path, max_articles=400000):\n",
    "    with open(file_path, \"r\", encoding='utf-8') as fi:\n",
    "        content = fi.read()\n",
    "\n",
    "    # Find all article titles and texts\n",
    "    titles = re.findall(\"== .* ==\", content)\n",
    "    texts = re.split(\"== .* == \\n\\n\", content)[1:]\n",
    "\n",
    "    # Convert zipped titles and texts to a list, then slice\n",
    "    articles = [{\"title\": ti.strip(\"==\").strip(), \"text\": tx.strip()} for ti, tx in list(zip(titles, texts))[:max_articles]]\n",
    "    return articles\n",
    "\n",
    "def turkish_lower(text):\n",
    "\n",
    "    text = text.replace('I', 'ı').replace('İ', 'i')\n",
    "    return text.lower()\n",
    "\n",
    "def filter_turkish_text(articles):\n",
    "    print('Processing articles...')\n",
    "    # Regex pattern for Turkish characters, excluding single characters\n",
    "    token_pattern = re.compile(r'\\b[a-zğüşıöç]{2,}(?:\\'[a-zğüşıöç]+)?\\b')\n",
    "\n",
    "    processed_articles = []\n",
    "\n",
    "    for article in articles:\n",
    "        text = turkish_lower(article['text'])\n",
    "        tokens = token_pattern.findall(text)\n",
    "        filtered_text = ' '.join(tokens)\n",
    "        processed_articles.append(filtered_text)\n",
    "\n",
    "    print(processed_articles[0])\n",
    "    print('Articles are processed.')\n",
    "    return processed_articles\n",
    "\n",
    "## Reading the articles\n",
    "training_set_path = drive_path + \"data/trwiki-67/trwiki-67.train.txt\"\n",
    "## This txt file is the preprocessed articles for the BPE tokenizer train. I did not include it because it is just the processed version of the training data.\n",
    "bpe_tokenizer_train_path = drive_path + 'bpe_training_text.txt' \n",
    "\n",
    "# Assuming you've already read the articles using your function\n",
    "articles = read_articles(training_set_path)\n",
    "\n",
    "processed_articles = filter_turkish_text(articles)\n",
    "\n",
    "# Write the filtered text bodies to a file with <s> and </s>\n",
    "with open(bpe_tokenizer_train_path, 'w', encoding='utf-8') as f:\n",
    "    for text in processed_articles:\n",
    "        f.write(\"<s> \" + text + \" </s>\\n\")\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"<OOV>\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Create a trainer\n",
    "trainer = BpeTrainer(special_tokens=[\"<OOV>\", \"<s>\", \"</s>\", \"<PAD>\"], vocab_size = 4000)\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train(files=[bpe_tokenizer_train_path], trainer=trainer)\n",
    "\n",
    "tokenizer.save(drive_path + \"embeddings/bpe_tokenizer_vocab.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3.2. Training the BPE Embeddings</h2>\n",
    "<h4>Content:</h4>\n",
    "<l>\n",
    "    <li>3.2.1. Defining the functions and training model</li>\n",
    "    <li>3.2.2. Preparing the training and validation dataset</li>\n",
    "    <li>3.2.3. Training loop</li>\n",
    "</l>\n",
    "\n",
    "\n",
    "<h3>3.2.1. Defining the functions and training model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tokenizers import Tokenizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SGD\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "import json\n",
    "\n",
    "def read_articles(file_path, max_articles=150000):\n",
    "    with open(file_path, \"r\", encoding='utf-8') as fi:\n",
    "        content = fi.read()\n",
    "\n",
    "    # Find all article titles and texts\n",
    "    titles = re.findall(\"== .* ==\", content)\n",
    "    texts = re.split(\"== .* == \\n\\n\", content)[1:]\n",
    "\n",
    "    # Convert zipped titles and texts to a list, then slice\n",
    "    articles = [{\"title\": ti.strip(\"==\").strip(), \"text\": tx.strip()} for ti, tx in list(zip(titles, texts))[:max_articles]]\n",
    "    return articles\n",
    "\n",
    "def turkish_lower(text):\n",
    "    \"\"\"\n",
    "    Convert text to lowercase, considering Turkish-specific characters.\n",
    "    \"\"\"\n",
    "    text = text.replace('I', 'ı').replace('İ', 'i')\n",
    "    return text.lower()\n",
    "\n",
    "def filter_turkish_text(articles):\n",
    "    print('Processing articles...')\n",
    "    # Regex pattern for Turkish characters, excluding single characters\n",
    "    token_pattern = re.compile(r'\\b[a-zğüşıöç]{2,}(?:\\'[a-zğüşıöç]+)?\\b')\n",
    "\n",
    "    processed_articles = []\n",
    "\n",
    "    stop_words = import_stop_words(drive_path + 'data/turkce-stop-words.txt')\n",
    "\n",
    "    for article in articles:\n",
    "        text = turkish_lower(article['text'])\n",
    "        words = token_pattern.findall(text)\n",
    "        for word in words:\n",
    "            if(is_stop_word(stop_words, word)):\n",
    "                words.remove(word)\n",
    "        processed_articles.append(words)\n",
    "    print(processed_articles[0])\n",
    "    print('Articles are processed.')\n",
    "    return processed_articles\n",
    "\n",
    "def create_tuples_from_articles(processed_articles, context_size, tokenizer):\n",
    "    context_target_pairs = []\n",
    "    skipped_pairs_count = 0\n",
    "    added_pairs_count = 0\n",
    "    for article in processed_articles:\n",
    "        for i in range(context_size, len(article) - context_size):\n",
    "            # Tokenize the context and target into BPE tokens\n",
    "            left_context_tokens = []\n",
    "            right_context_tokens = []\n",
    "            for j in range(i - context_size, i):\n",
    "                left_context_tokens.extend(tokenizer.encode(article[j]).ids)\n",
    "            for j in range(i + 1, i + context_size + 1):\n",
    "                right_context_tokens.extend(tokenizer.encode(article[j]).ids)\n",
    "\n",
    "            target_tokens = tokenizer.encode(article[i]).ids\n",
    "\n",
    "            # For each token in the target, create a new context-target pair\n",
    "            for target_token in target_tokens:\n",
    "                # Combine left context, other tokens of the target word, and right context\n",
    "                extended_context = left_context_tokens + \\\n",
    "                                   [token for token in target_tokens if token != target_token] + \\\n",
    "                                   right_context_tokens\n",
    "\n",
    "                # Check if the context or target is empty before appending\n",
    "                if extended_context and target_token:\n",
    "                    context_target_pairs.append((extended_context, target_token))\n",
    "                    added_pairs_count += 1\n",
    "                else:\n",
    "                    skipped_pairs_count += 1\n",
    "    print('Skipped pairs: ' + str(skipped_pairs_count))\n",
    "    print('Added pairs: ' + str(added_pairs_count))\n",
    "    return context_target_pairs\n",
    "\n",
    "\n",
    "\n",
    "def import_stop_words(file_path):\n",
    "    stop_words = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            stop_words.append(line.strip())  # .strip() removes any trailing newlines or spaces\n",
    "    return stop_words\n",
    "\n",
    "def is_stop_word(stop_words,word):\n",
    "    return word in stop_words\n",
    "\n",
    "def get_collate_fn(pad_token_id):\n",
    "    def collate_batch(batch):\n",
    "        contexts, targets = zip(*batch)\n",
    "\n",
    "        # Since contexts are already tensors, use them directly in pad_sequence\n",
    "        contexts_padded = pad_sequence(contexts, batch_first=True, padding_value=pad_token_id)\n",
    "\n",
    "        # Convert targets list to a tensor\n",
    "        # Assuming targets are already tensors as per your dataset class\n",
    "        targets = torch.stack(targets)\n",
    "\n",
    "        return contexts_padded, targets\n",
    "\n",
    "    return collate_batch\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, max_norm = 1.0)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds_mean = embeds.mean(dim=1)\n",
    "        out = self.linear(embeds_mean)\n",
    "        return out\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, context_target_pairs):\n",
    "        self.context_target_pairs = context_target_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.context_target_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.context_target_pairs[idx]\n",
    "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.2.2. Preparing the training and validation dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the articles\n",
    "training_set_path = drive_path + \"data/trwiki-67/trwiki-67.train.txt\"\n",
    "validation_set_path = drive_path + \"data/trwiki-67/trwiki-67.val.txt\"\n",
    "tokenizer_path = drive_path + 'embeddings/bpe_tokenizer_vocab4000_with_pad.json'  # Path to the saved tokenizer\n",
    "\n",
    "## Reading the tokenizer.\n",
    "bpe_tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "training_articles = filter_turkish_text(read_articles(training_set_path))\n",
    "validation_articles = filter_turkish_text(read_articles(validation_set_path))\n",
    "CONTEXT_SIZE = 2\n",
    "training_articles = create_tuples_from_articles(training_articles, CONTEXT_SIZE, bpe_tokenizer)\n",
    "validation_articles = create_tuples_from_articles(validation_articles, CONTEXT_SIZE, bpe_tokenizer)\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = CBOWDataset(training_articles)\n",
    "val_dataset = CBOWDataset(validation_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.2.3. Training loop</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 512\n",
    "learning_rate = 0.08\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "vocab_size = len(bpe_tokenizer.get_vocab())\n",
    "\n",
    "print('Vocab size : ' + str(vocab_size))\n",
    "\n",
    "pad_token_id = bpe_tokenizer.token_to_id(\"<PAD>\")\n",
    "collate_fn = get_collate_fn(pad_token_id)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# Model, Device, DataLoader\n",
    "neural_net = CBOW(vocab_size, embedding_dim)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "neural_net.to(device)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(neural_net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "print('Starting training')\n",
    "for epoch in range(epochs):\n",
    "    neural_net.train()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "\n",
    "    for context_vector, target_index in progress_bar:\n",
    "        context_vector, target_index = context_vector.to(device), target_index.to(device)\n",
    "\n",
    "        neural_net.zero_grad()\n",
    "        log_probs = neural_net(context_vector)\n",
    "        loss = loss_function(log_probs, target_index)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f'\\nEpoch {epoch+1} Completed - Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    # Validation\n",
    "    neural_net.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for context_vector, target_index in val_dataloader:\n",
    "            context_vector, target_index = context_vector.to(device), target_index.to(device)\n",
    "            log_probs = neural_net(context_vector)\n",
    "            loss = loss_function(log_probs, target_index)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Save model\n",
    "    model_save_path = drive_path + f'bpe_model_epoch_{epoch+1}.pth'\n",
    "    torch.save(neural_net.state_dict(), model_save_path)\n",
    "\n",
    "# Optionally, save the final model separatelys\n",
    "final_model_save_path = drive_path + 'bpe_model_final.pth'\n",
    "torch.save(neural_net.state_dict(), final_model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
