There are 3 embeddings files compressed into zip files separately. Since the vocabulary size is very large, I am submitting the compressed versions of the .txt files. The embedding dimension for all tokenizers is 512. Vocabularies of the tokenizers is also provided as .json files.

Vocabulary sizes:
1) Word Tokenizer: 50K tokens
2) Trigram Tokenizer: 27125 tokens
3) BPE Tokenizer: 4K tokens

The file format for .txt files is as follows:

'token1' : [VECTOR1]
'token2' : [VECTOR2]
...

Oğuz Kağan Hitit
0076757

