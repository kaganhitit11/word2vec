{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Word Tokenizer Evaluation</h1>\n",
    "<h4>Content:</h4>\n",
    "<l>\n",
    "    <li>1.1. Defining the evaluation functions and the training model</li>\n",
    "    <li>1.2. Defining the model parameters and loading the model</li>\n",
    "    <li>1.3. Syntactic and Semantic Evaluation</li>\n",
    "</l>\n",
    "\n",
    "<h3>1.1. Defining the evaluation functions and the training model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tokenizers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tokenizers'"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "def create_word_embedding(word, vocabulary, model):\n",
    "    if word in vocabulary:\n",
    "        word_index = vocabulary[word]\n",
    "    else:\n",
    "        return torch.zeros(model.embeddings.embedding_dim)\n",
    "    token_ids = torch.tensor([word_index], dtype=torch.long)\n",
    "    embedding = model.embeddings(token_ids)\n",
    "    embedding = embedding.sum(dim=0)\n",
    "    return embedding\n",
    "\n",
    "def load_vocabulary(vocab_path):\n",
    "  with open(vocab_path, 'r', encoding='utf-8') as f:\n",
    "      vocab = json.load(f)\n",
    "  return vocab\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, max_norm=1.0)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds_mean = embeds.mean(dim=1)\n",
    "        out = self.linear(embeds_mean)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.2. Defining the model parameters and loading the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constants\n",
    "parent_path = '/Users/kaganhitit_/Desktop/COMP442/0076757_word2vec/starter/'\n",
    "\n",
    "unique_words_path = parent_path + 'evaluation/evaluation_training_unique_words.txt'\n",
    "syntactic_file_path = parent_path + 'data/SynAnalogyTr.txt'  # Replace with the actual path\n",
    "semantic_file_path = parent_path + 'data/turkish-analogy-semantic.txt'  # Replace with the actual path\n",
    "vocab_path = parent_path + 'embeddings/word_tokenizer_vocab.json'\n",
    "\n",
    "\n",
    "## Parameters\n",
    "vocabulary = load_vocabulary(vocab_path)\n",
    "vocab_size = len(vocabulary.keys())\n",
    "embedding_dim = 512\n",
    "model_name = 'word_model_final.pth'\n",
    "embeddings_path = model_name.rsplit('.', 1)[0] + '_embeddings.pth'\n",
    "\n",
    "model = CBOW(vocab_size, embedding_dim)\n",
    "model.load_state_dict(torch.load(parent_path + model_name, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "unique_words = []\n",
    "with open(unique_words_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        word = line.strip() \n",
    "        unique_words.append(word)\n",
    "\n",
    "print('Unique word length :' + str(len(unique_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.3. Syntactic and Semantic Evaluation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "\n",
    "# Assuming unique_words is a list of unique words\n",
    "for word in unique_words:\n",
    "    embeddings_dict[word] = create_word_embedding(word, vocabulary, model).detach().numpy()\n",
    "\n",
    "torch.save(embeddings_dict, embeddings_path)\n",
    "\n",
    "print(\"Embeddings saved to\", embeddings_path)\n",
    "\n",
    "embeddings_dict = torch.load(embeddings_path)\n",
    "\n",
    "embeddings_matrix = np.zeros((len(embeddings_dict), embedding_dim))\n",
    "\n",
    "for i, (word, embedding) in enumerate(embeddings_dict.items()):\n",
    "    embeddings_matrix[i, :] = embedding\n",
    "embeddings_matrix = np.array([emb / norm(emb) if norm(emb) > 0 else emb for emb in embeddings_matrix])\n",
    "print(\"Embeddings matrix shape:\", embeddings_matrix.shape)\n",
    "\n",
    "\n",
    "syntactic_data = []\n",
    "# Path to your SynAnalogyTr.txt file\n",
    "\n",
    "with open(syntactic_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        words = line.strip().split()\n",
    "        if len(words) == 4:\n",
    "            syntactic_data.append(words)\n",
    "\n",
    "target_embeddings = np.zeros(shape=(len(syntactic_data), 512))\n",
    "\n",
    "for i in range(len(syntactic_data)):\n",
    "  a = create_word_embedding(syntactic_data[i][0], vocabulary, model)\n",
    "  b = create_word_embedding(syntactic_data[i][1], vocabulary, model)\n",
    "  c = create_word_embedding(syntactic_data[i][2], vocabulary, model)\n",
    "  target_embedding = (b-a+c).detach().numpy()\n",
    "  target_embedding /= norm(target_embedding)\n",
    "  target_embeddings[i] = target_embedding\n",
    "\n",
    "target_embeddings = target_embeddings.transpose()\n",
    "print(\"Target embeddings matrix shape:\", target_embeddings.shape)\n",
    "\n",
    "cosine_similarities = np.matmul(embeddings_matrix, target_embeddings)\n",
    "\n",
    "top_5_indices_all_columns = []\n",
    "for column in range(cosine_similarities.shape[1]):\n",
    "    top_5_indices = np.argsort(cosine_similarities[:, column])[-5:][::-1]\n",
    "    top_5_indices_all_columns.append(top_5_indices)\n",
    "\n",
    "words_list = list(embeddings_dict.keys())\n",
    "\n",
    "syntactic_accuracy = 0\n",
    "accurate_words = []\n",
    "MRR_syntactic = 0\n",
    "\n",
    "for i in range(len(top_5_indices_all_columns)):\n",
    "    d = syntactic_data[i][3]\n",
    "    rank = 0\n",
    "    for j, idx in enumerate(top_5_indices_all_columns[i]):\n",
    "        word_at_index = words_list[idx]\n",
    "        if word_at_index == d:\n",
    "            syntactic_accuracy += 1\n",
    "            accurate_words.append(d)\n",
    "            rank = j + 1\n",
    "            break\n",
    "    if rank > 0:\n",
    "        MRR_syntactic += 1 / rank\n",
    "\n",
    "syntactic_accuracy /= len(syntactic_data)\n",
    "MRR_syntactic /= len(syntactic_data)\n",
    "\n",
    "print('Syntactic accuracy:', syntactic_accuracy)\n",
    "print('Syntactic MRR:', MRR_syntactic)\n",
    "print('Accurate words:', str(accurate_words))\n",
    "\n",
    "## Semantic Accuracy\n",
    "semantic_data = {}\n",
    "current_category = None\n",
    "with open(semantic_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line.startswith(':'):\n",
    "            current_category = line[1:].strip()  # Remove the colon at the start and strip spaces\n",
    "            semantic_data[current_category] = []\n",
    "        elif current_category is not None:\n",
    "            words = line.split()\n",
    "            if len(words) == 4:\n",
    "                semantic_data[current_category].append(words)\n",
    "\n",
    "\n",
    "for category_name in semantic_data.keys():\n",
    "\n",
    "    category = semantic_data[category_name]\n",
    "\n",
    "    target_embeddings = np.zeros(shape=(len(category), 512))\n",
    "\n",
    "    for i in range(len(category)):\n",
    "        a = create_word_embedding(category[i][0], vocabulary, model)\n",
    "        b = create_word_embedding(category[i][1], vocabulary, model)\n",
    "        c = create_word_embedding(category[i][2], vocabulary, model)\n",
    "        target_embedding = (b-a+c).detach().numpy()\n",
    "        target_embedding /= norm(target_embedding)\n",
    "        target_embeddings[i] = target_embedding\n",
    "\n",
    "    target_embeddings = target_embeddings.transpose()\n",
    "    print(category_name + \", Target embeddings matrix shape:\" + str(target_embeddings.shape))\n",
    "\n",
    "    cosine_similarities = np.matmul(embeddings_matrix, target_embeddings)\n",
    "\n",
    "    top_5_indices_all_columns = []\n",
    "    for column in range(cosine_similarities.shape[1]):\n",
    "        top_5_indices = np.argsort(cosine_similarities[:, column])[-5:][::-1]\n",
    "        top_5_indices_all_columns.append(top_5_indices)\n",
    "\n",
    "    words_list = list(embeddings_dict.keys())\n",
    "\n",
    "    semantic_accuracy = 0\n",
    "    MRR_semantic = 0\n",
    "    accurate_words = []\n",
    "    for i in range(len(top_5_indices_all_columns)):\n",
    "        d = category[i][3]\n",
    "        rank = 0\n",
    "        for j, idx in enumerate(top_5_indices_all_columns[i]):\n",
    "            word_at_index = words_list[idx]\n",
    "            if word_at_index == d:\n",
    "                semantic_accuracy += 1\n",
    "                accurate_words.append(d)\n",
    "                rank = j + 1\n",
    "                break\n",
    "        if rank > 0:\n",
    "            MRR_semantic += 1 / rank\n",
    "\n",
    "    semantic_accuracy /= len(category)\n",
    "    MRR_semantic /= len(category)\n",
    "\n",
    "    print(str(category_name) + ' semantic accuracy:', semantic_accuracy)\n",
    "    print(str(category_name) + ' semantic MRR:', MRR_semantic)\n",
    "    print('Accurate words:', str(accurate_words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Character Trigram Tokenizer Evaluation</h1>\n",
    "<h4>Content:</h4>\n",
    "<l>\n",
    "    <li>2.1. Defining the evaluation functions and the training model</li>\n",
    "    <li>2.2. Defining the model parameters and loading the model</li>\n",
    "    <li>2.3. Syntactic and Semantic Evaluation</li>\n",
    "</l>\n",
    "\n",
    "<h3>2.1. Defining the evaluation functions and the training model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "def create_word_embedding(word, vocabulary, model):\n",
    "    word = '<' + word + '>'\n",
    "    trigrams = [word[i:i+3] for i in range(len(word) - 2)]\n",
    "    encoded_trigrams = [vocabulary.get(trigram, vocabulary.get('<OOV>')) for trigram in trigrams]\n",
    "    token_ids = torch.tensor(encoded_trigrams, dtype=torch.long)\n",
    "    embedding = model.embeddings(token_ids)\n",
    "    embedding = embedding.sum(dim=0)\n",
    "    return embedding\n",
    "\n",
    "def load_vocabulary(vocab_path):\n",
    "  with open(vocab_path, 'r', encoding='utf-8') as f:\n",
    "      vocab = json.load(f)\n",
    "  return vocab\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, max_norm=1.0)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds_mean = embeds.mean(dim=1)\n",
    "        out = self.linear(embeds_mean)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.2. Defining the model parameters and loading the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constants\n",
    "## parent_path = 'path/to/your/directory/'\n",
    "parent_path = '/Users/kaganhitit_/Desktop/COMP442/0076757_word2vec/starter/'\n",
    "unique_words_path = parent_path + 'evaluation/evaluation_training_unique_words.txt'\n",
    "syntactic_file_path = parent_path + 'data/SynAnalogyTr.txt'  # Replace with the actual path\n",
    "semantic_file_path = parent_path +'data/turkish-analogy-semantic.txt'  # Replace with the actual path\n",
    "vocab_path = parent_path + 'embeddings/trigram_tokenizer_vocab.json'\n",
    "\n",
    "\n",
    "## Parameters\n",
    "vocabulary = load_vocabulary(vocab_path)\n",
    "vocab_size = len(vocabulary.keys())\n",
    "embedding_dim = 512\n",
    "model_name = 'trigram_model_final.pth'\n",
    "embeddings_path = model_name.rsplit('.', 1)[0] + '_embeddings.pth'\n",
    "\n",
    "model = CBOW(vocab_size, embedding_dim)\n",
    "model.load_state_dict(torch.load(parent_path + model_name, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "unique_words = []\n",
    "with open(unique_words_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        word = line.strip()\n",
    "        unique_words.append(word)\n",
    "\n",
    "print('Unique word length :' + str(len(unique_words)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.3. Syntactic and Semantic Evaluation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "\n",
    "# Assuming unique_words is a list of unique words\n",
    "for word in unique_words:\n",
    "    embeddings_dict[word] = create_word_embedding(word, vocabulary, model).detach().numpy()\n",
    "\n",
    "# Saving the embeddings\n",
    "torch.save(embeddings_dict, embeddings_path)\n",
    "\n",
    "print(\"Embeddings saved to\", embeddings_path)\n",
    "\n",
    "embeddings_dict = torch.load(embeddings_path)\n",
    "\n",
    "# Initialize the embeddings matrix\n",
    "embeddings_matrix = np.zeros((len(embeddings_dict), embedding_dim))\n",
    "\n",
    "# Fill the matrix with embeddings\n",
    "for i, (word, embedding) in enumerate(embeddings_dict.items()):\n",
    "    embeddings_matrix[i, :] = embedding\n",
    "embeddings_matrix = np.array([emb / norm(emb) if norm(emb) > 0 else emb for emb in embeddings_matrix])\n",
    "print(\"Embeddings matrix shape:\", embeddings_matrix.shape)\n",
    "\n",
    "\n",
    "syntactic_data = []\n",
    "# Path to your SynAnalogyTr.txt file\n",
    "\n",
    "# Read the file\n",
    "with open(syntactic_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        words = line.strip().split()\n",
    "        if len(words) == 4:\n",
    "            syntactic_data.append(words)\n",
    "\n",
    "target_embeddings = np.zeros(shape=(len(syntactic_data), 512))\n",
    "\n",
    "for i in range(len(syntactic_data)):\n",
    "  a = create_word_embedding(syntactic_data[i][0], vocabulary, model)\n",
    "  b = create_word_embedding(syntactic_data[i][1], vocabulary, model)\n",
    "  c = create_word_embedding(syntactic_data[i][2], vocabulary, model)\n",
    "  target_embedding = (b-a+c).detach().numpy()\n",
    "  target_embedding /= norm(target_embedding)\n",
    "  target_embeddings[i] = target_embedding\n",
    "\n",
    "target_embeddings = target_embeddings.transpose()\n",
    "print(\"Target embeddings matrix shape:\", target_embeddings.shape)\n",
    "\n",
    "# Compute cosine similarities\n",
    "cosine_similarities = np.matmul(embeddings_matrix, target_embeddings)\n",
    "\n",
    "top_5_indices_all_columns = []\n",
    "for column in range(cosine_similarities.shape[1]):\n",
    "    top_5_indices = np.argsort(cosine_similarities[:, column])[-5:][::-1]\n",
    "    top_5_indices_all_columns.append(top_5_indices)\n",
    "\n",
    "words_list = list(embeddings_dict.keys())\n",
    "\n",
    "syntactic_accuracy = 0\n",
    "accurate_words = []\n",
    "MRR_syntactic = 0\n",
    "\n",
    "for i in range(len(top_5_indices_all_columns)):\n",
    "    d = syntactic_data[i][3]\n",
    "    rank = 0\n",
    "    for j, idx in enumerate(top_5_indices_all_columns[i]):\n",
    "        word_at_index = words_list[idx]\n",
    "        if word_at_index == d:\n",
    "            syntactic_accuracy += 1\n",
    "            accurate_words.append(d)\n",
    "            rank = j + 1\n",
    "            break\n",
    "    if rank > 0:\n",
    "        MRR_syntactic += 1 / rank\n",
    "\n",
    "syntactic_accuracy /= len(syntactic_data)\n",
    "MRR_syntactic /= len(syntactic_data)\n",
    "\n",
    "print('Syntactic accuracy:', syntactic_accuracy)\n",
    "print('Syntactic MRR:', MRR_syntactic)\n",
    "print('Accurate words:', str(accurate_words))\n",
    "\n",
    "## Semantic Accuracy\n",
    "semantic_data = {}\n",
    "current_category = None\n",
    "# Read the file\n",
    "with open(semantic_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line.startswith(':'):\n",
    "            current_category = line[1:].strip()\n",
    "            semantic_data[current_category] = []\n",
    "        elif current_category is not None:\n",
    "            words = line.split()\n",
    "            if len(words) == 4:\n",
    "                semantic_data[current_category].append(words)\n",
    "\n",
    "\n",
    "for category_name in semantic_data.keys():\n",
    "\n",
    "    category = semantic_data[category_name]\n",
    "\n",
    "    target_embeddings = np.zeros(shape=(len(category), 512))\n",
    "\n",
    "    for i in range(len(category)):\n",
    "        a = create_word_embedding(category[i][0], vocabulary, model)\n",
    "        b = create_word_embedding(category[i][1], vocabulary, model)\n",
    "        c = create_word_embedding(category[i][2], vocabulary, model)\n",
    "        target_embedding = (b-a+c).detach().numpy()\n",
    "        target_embedding /= norm(target_embedding)\n",
    "        target_embeddings[i] = target_embedding\n",
    "\n",
    "    target_embeddings = target_embeddings.transpose()\n",
    "    print(category_name + \", Target embeddings matrix shape:\" + str(target_embeddings.shape))\n",
    "\n",
    "    cosine_similarities = np.matmul(embeddings_matrix, target_embeddings)\n",
    "\n",
    "    top_5_indices_all_columns = []\n",
    "    for column in range(cosine_similarities.shape[1]):\n",
    "        top_5_indices = np.argsort(cosine_similarities[:, column])[-5:][::-1]\n",
    "        top_5_indices_all_columns.append(top_5_indices)\n",
    "\n",
    "    words_list = list(embeddings_dict.keys())\n",
    "\n",
    "    semantic_accuracy = 0\n",
    "    MRR_semantic = 0\n",
    "    accurate_words = []\n",
    "    for i in range(len(top_5_indices_all_columns)):\n",
    "        d = category[i][3]\n",
    "        rank = 0\n",
    "        for j, idx in enumerate(top_5_indices_all_columns[i]):\n",
    "            word_at_index = words_list[idx]\n",
    "            if word_at_index == d:\n",
    "                semantic_accuracy += 1\n",
    "                accurate_words.append(d)\n",
    "                rank = j + 1\n",
    "                break\n",
    "        if rank > 0:\n",
    "            MRR_semantic += 1 / rank\n",
    "\n",
    "    semantic_accuracy /= len(category)\n",
    "    MRR_semantic /= len(category)\n",
    "\n",
    "    print(str(category_name) + ' semantic accuracy:', semantic_accuracy)\n",
    "    print(str(category_name) + ' semantic MRR:', MRR_semantic)\n",
    "    print('Accurate words:', str(accurate_words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. BPE Tokenizer Evaluation</h1>\n",
    "<h4>Content:</h4>\n",
    "<l>\n",
    "    <li>3.1. Defining the evaluation functions and the training model</li>\n",
    "    <li>3.2. Defining the model parameters and loading the model</li>\n",
    "    <li>3.3. Syntactic and Semantic Evaluation</li>\n",
    "</l>\n",
    "\n",
    "<h3>3.1. Defining the evaluation functions and the training model</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "def create_word_embedding(word, bpe_tokenizer, model):\n",
    "    encoded = bpe_tokenizer.encode(word)\n",
    "    # Ensure the tensor is of type Long\n",
    "    token_ids = torch.tensor(encoded.ids, dtype=torch.long)\n",
    "    embedding = model.embeddings(token_ids)\n",
    "    embedding = embedding.sum(dim=0)\n",
    "    return embedding\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, max_norm=1.0)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds_mean = embeds.mean(dim=1)\n",
    "        out = self.linear(embeds_mean)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.2. Defining the model parameters and loading the model</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "def create_word_embedding(word, bpe_tokenizer, embeddings_dict):\n",
    "    encoded = bpe_tokenizer.encode(word)\n",
    "    # Ensure the tensor is of type Long\n",
    "    token_ids = encoded.ids\n",
    "    embedding_sum = np.sum([embeddings_dict.get(sub_token, np.zeros(embedding_dim)) for sub_token in token_ids], axis=0)\n",
    "    return embedding_sum\n",
    "\n",
    "## Constants\n",
    "## parent_path = 'path/to/your/directory/'\n",
    "parent_path = '/Users/kaganhitit_/Desktop/COMP442/0076757_word2vec/starter/'\n",
    "unique_words_path = parent_path + 'evaluation/evaluation_training_unique_words.txt'\n",
    "syntactic_file_path = parent_path + 'data/SynAnalogyTr.txt'  # Replace with the actual path\n",
    "semantic_file_path = parent_path +'data/turkish-analogy-semantic.txt'  # Replace with the actual path\n",
    "\n",
    "## Parameters\n",
    "vocab_size = 4000\n",
    "embedding_dim = 512\n",
    "tokenizer_name = 'embeddings/bpe_tokenizer_vocab.json'\n",
    "embeddings_path  = parent_path + 'embeddings/bpe_tokenizer_embeddings.txt'\n",
    "\n",
    "bpe_tokenizer = Tokenizer.from_file(parent_path + tokenizer_name)\n",
    "\n",
    "embeddings_dict = {}\n",
    "\n",
    "with open(embeddings_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        token, vector = line.split(':', 1)\n",
    "        vector = json.loads(vector)\n",
    "        embeddings_dict[token.strip()] = np.array(vector)\n",
    "\n",
    "print(\"Loaded embeddings for\", len(embeddings_dict), \"tokens.\")\n",
    "\n",
    "unique_words = []\n",
    "with open(unique_words_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        word = line.strip()  \n",
    "        unique_words.append(word)\n",
    "\n",
    "print('Unique word length :' + str(len(unique_words)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.3. Syntactic and Semantic Evaluation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "\n",
    "for word in unique_words:\n",
    "    embeddings_dict[word] = create_word_embedding(word, bpe_tokenizer, embeddings_dict)\n",
    "\n",
    "embeddings_matrix = np.zeros((len(embeddings_dict), embedding_dim))\n",
    "\n",
    "for i, (word, embedding) in enumerate(embeddings_dict.items()):\n",
    "    embeddings_matrix[i, :] = embedding\n",
    "embeddings_matrix = np.array([emb / norm(emb) if norm(emb) > 0 else emb for emb in embeddings_matrix])\n",
    "print(\"Embeddings matrix shape:\", embeddings_matrix.shape)\n",
    "\n",
    "\n",
    "syntactic_data = []\n",
    "\n",
    "with open(syntactic_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        words = line.strip().split()\n",
    "        if len(words) == 4:\n",
    "            syntactic_data.append(words)\n",
    "\n",
    "target_embeddings = np.zeros(shape=(len(syntactic_data), 512))\n",
    "\n",
    "for i in range(len(syntactic_data)):\n",
    "  a = create_word_embedding(syntactic_data[i][0], bpe_tokenizer, embeddings_dict)\n",
    "  b = create_word_embedding(syntactic_data[i][1], bpe_tokenizer, embeddings_dict)\n",
    "  c = create_word_embedding(syntactic_data[i][2], bpe_tokenizer, embeddings_dict)\n",
    "  target_embedding = (b-a+c)\n",
    "  target_embedding /= norm(target_embedding)\n",
    "  target_embeddings[i] = target_embedding\n",
    "\n",
    "target_embeddings = target_embeddings.transpose()\n",
    "print(\"Target embeddings matrix shape:\", target_embeddings.shape)\n",
    "\n",
    "cosine_similarities = np.matmul(embeddings_matrix, target_embeddings)\n",
    "\n",
    "top_5_indices_all_columns = []\n",
    "for column in range(cosine_similarities.shape[1]):\n",
    "    top_5_indices = np.argsort(cosine_similarities[:, column])[-5:][::-1]\n",
    "    top_5_indices_all_columns.append(top_5_indices)\n",
    "\n",
    "words_list = list(embeddings_dict.keys())\n",
    "\n",
    "syntactic_accuracy = 0\n",
    "accurate_words = []\n",
    "MRR_syntactic = 0\n",
    "\n",
    "for i in range(len(top_5_indices_all_columns)):\n",
    "    d = syntactic_data[i][3]\n",
    "    rank = 0\n",
    "    for j, idx in enumerate(top_5_indices_all_columns[i]):\n",
    "        word_at_index = words_list[idx]\n",
    "        if word_at_index == d:\n",
    "            syntactic_accuracy += 1\n",
    "            accurate_words.append(d)\n",
    "            rank = j + 1\n",
    "            break\n",
    "    if rank > 0:\n",
    "        MRR_syntactic += 1 / rank\n",
    "\n",
    "syntactic_accuracy /= len(syntactic_data)\n",
    "MRR_syntactic /= len(syntactic_data)\n",
    "\n",
    "print('Syntactic accuracy:', syntactic_accuracy)\n",
    "print('Syntactic MRR:', MRR_syntactic)\n",
    "print('Accurate words:', str(accurate_words))\n",
    "\n",
    "## Semantic Accuracy\n",
    "semantic_data = {}\n",
    "current_category = None\n",
    "with open(semantic_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line.startswith(':'):\n",
    "            current_category = line[1:].strip() \n",
    "            semantic_data[current_category] = []\n",
    "        elif current_category is not None:\n",
    "            words = line.split()\n",
    "            if len(words) == 4:\n",
    "                semantic_data[current_category].append(words)\n",
    "\n",
    "\n",
    "for category_name in semantic_data.keys():\n",
    "\n",
    "    category = semantic_data[category_name]\n",
    "\n",
    "    target_embeddings = np.zeros(shape=(len(category), 512))\n",
    "\n",
    "    for i in range(len(category)):\n",
    "        a = create_word_embedding(category[i][0], bpe_tokenizer, embeddings_dict)\n",
    "        b = create_word_embedding(category[i][1], bpe_tokenizer, embeddings_dict)\n",
    "        c = create_word_embedding(category[i][2], bpe_tokenizer, embeddings_dict)\n",
    "        target_embedding = (b-a+c)\n",
    "        target_embedding /= norm(target_embedding)\n",
    "        target_embeddings[i] = target_embedding\n",
    "\n",
    "    target_embeddings = target_embeddings.transpose()\n",
    "    print(category_name + \", Target embeddings matrix shape:\" + str(target_embeddings.shape))\n",
    "\n",
    "    cosine_similarities = np.matmul(embeddings_matrix, target_embeddings)\n",
    "\n",
    "    top_5_indices_all_columns = []\n",
    "    for column in range(cosine_similarities.shape[1]):\n",
    "        top_5_indices = np.argsort(cosine_similarities[:, column])[-5:][::-1]\n",
    "        top_5_indices_all_columns.append(top_5_indices)\n",
    "\n",
    "    words_list = list(embeddings_dict.keys())\n",
    "\n",
    "    semantic_accuracy = 0\n",
    "    MRR_semantic = 0\n",
    "    accurate_words = []\n",
    "    for i in range(len(top_5_indices_all_columns)):\n",
    "        d = category[i][3]\n",
    "        rank = 0\n",
    "        for j, idx in enumerate(top_5_indices_all_columns[i]):\n",
    "            word_at_index = words_list[idx]\n",
    "            if word_at_index == d:\n",
    "                semantic_accuracy += 1\n",
    "                accurate_words.append(d)\n",
    "                rank = j + 1\n",
    "                break\n",
    "        if rank > 0:\n",
    "            MRR_semantic += 1 / rank\n",
    "\n",
    "    semantic_accuracy /= len(category)\n",
    "    MRR_semantic /= len(category)\n",
    "\n",
    "    print(str(category_name) + ' semantic accuracy:', semantic_accuracy)\n",
    "    print(str(category_name) + ' semantic MRR:', MRR_semantic)\n",
    "    print('Accurate words:', str(accurate_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
